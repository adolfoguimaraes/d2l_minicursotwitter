{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minicurso: O que o twitter está pensando?\n",
    "\n",
    "Estes tutoriais apresentam os principais scritps desenvolvidos no minicurso: **O que o twitter está pensando? Extraindo informações em redes sociais utilizando Python**. Os arquivos completos dos scritps e códigos gerados podem ser encontrados nas pastas **scritps** e **web** na raiz do repositório.\n",
    "\n",
    "A apresentação referente a este minicurso está disponível no site: http://www.data2learning.com/cursos.\n",
    "\n",
    "## 04 - Juntando tudo\n",
    "\n",
    "Ao longo deste tutorial aprendemos como selecionar dados do twitter e fazer algumas as etapas de pré-processamento. O objetivo desta seção é juntar tudo. Vamos coletar dados do twitter, pré-processar e imprimir a lista de tokens mais frequentes para um conjunto de dados coletados.\n",
    "\n",
    "* O primeiro passo é coletar tweets e adicionar em uma lista. Para nossa tarefa só interessa o texto e o login do usuário que está postando. Demais informações podem ser descartadas.\n",
    "* Em seguida vamos processar os textos coletados tokenizando.\n",
    "* Deve ser armazenado uma lista das hashtags mais postadas e dos usuários que mais postaram.\n",
    "* Em seguida devemos eliminar as stopwords.\n",
    "* Devemos eliminar eventuais problemas no texto: palavras repetidas e escritas de forma incorreta.\n",
    "* Por fim, extrair os 50 termos mais frequentes usando bigram e trigram.\n",
    "* Ao final deve imprimir a lista dos termos, a lista das hashtags e a lista dos usuários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "\n",
    "from twython import Twython\n",
    "from datetime import datetime\n",
    "\n",
    "# Definindo as chaves do Twitter\n",
    "\n",
    "APP_KEY = \"0rBTp9a35qIvA5ufGpxPGNkWu\"\n",
    "APP_SECRET = \"rGStqnwrDjuzo1zwnXjpPlrilvDmvNljhRh6cTs1pG48K6ZLG6\"\n",
    "OAUTH_TOKEN = \"736392442384154624-blYbsB4awwSezrNUH7L5jTG6JPglJy3\"\n",
    "OAUTH_TOKEN_SECRET = \"bvPr7Y8BQeyN46UKZtDJPyP0Bx4Y8IuDRDVYxsc3LNAlb\"\n",
    "\n",
    "tw = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "result = tw.search(q=\"futebol\",count=100,lang=\"pt\")\n",
    "\n",
    "tweets = result['statuses']\n",
    "\n",
    "list_text = []\n",
    "list_user = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    #Evita pegar texto truncado no caso de Retweeteds\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        list_text.append(tweet['retweeted_status']['text'])\n",
    "    else:\n",
    "        list_text.append(tweet['text'])\n",
    "        \n",
    "    list_user.append(tweet['user']['screen_name'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As variáveis **list_text** e **list_user** armazenam a lista de todos os textos e usuários, respectivamente. O passo seguinte é tratar essas listas com as etapas de pré-processamento.\n",
    "\n",
    "Vamos tratar todos os tweets como um único texto e fazer o pré-processamento nesse texto. A partir desse texto vamos extrair a lista palavras mais citadas, lista de usuários citados e lista de hashtags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unicodedata import normalize, category\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from collections import Counter, Set\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "pattern = r'(https://[^\"\\' ]+|www.[^\"\\' ]+|http://[^\"\\' ]+|\\w+|\\@\\w+|\\#\\w+)'\n",
    "portuguese_stops = stopwords.words(['portuguese'])\n",
    "\n",
    "users_cited = []\n",
    "links_appears = []\n",
    "hashtags = []\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for text in list_text:\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    patterns += regexp_tokenize(text, pattern)\n",
    "\n",
    "    users_cited += [e for e in patterns if e[0] == '@']\n",
    "    links_appears += [e for e in patterns if e[:4] == 'http']\n",
    "    hashtags += [e for e in patterns if e[0] == '#']\n",
    "    \n",
    "    final_tokens = [e for e in patterns if e[:4] != 'http']\n",
    "    final_tokens = [e for e in final_tokens if e[:4] != 'www.']\n",
    "    final_tokens = [e for e in final_tokens if e[0] != '#']\n",
    "    final_tokens = [e for e in final_tokens if e[0] != '@']\n",
    "    \n",
    "    \n",
    "words = [word for word in final_tokens if word not in portuguese_stops]\n",
    "\n",
    "word_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classes para correção\n",
    "import re\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "    \n",
    "    def __init__(self, dict_name='pt_BR',max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "        \n",
    "    def replace(self, word):\n",
    "        \n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        \n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        \n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cria um map das palavras erradas para as palavras corrigidas\n",
    "\n",
    "replacer_spelling = SpellingReplacer()\n",
    "replacer_repeat = RepeatReplacer()\n",
    "\n",
    "new_words = []\n",
    "map_words = {}\n",
    "for word in word_set:\n",
    "    \n",
    "    new_word = replacer_repeat.replace(word)\n",
    "    new_word = replacer_spelling.replace(new_word)\n",
    "    \n",
    "    map_words[word] = new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_temp = [map_words[word] for word in words ]\n",
    "words_temp = [word for word in words_temp if len(word) >= 3]\n",
    "\n",
    "final_words = []\n",
    "\n",
    "for word in words_temp:\n",
    "    try:\n",
    "        new_word = normalize('NFKD', word.lower()).encode('ASCII','ignore')\n",
    "    except UnicodeEncodeError:\n",
    "        new_word = normalize('NFKD', word.lower().decode('utf-8')).encode('ASCII','ignore')\n",
    "\n",
    "    final_words.append(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "futebol\n",
      "transito\n",
      "comemorar\n",
      "rio\n",
      "galera\n",
      "pleno\n",
      "abriu\n",
      "leilton\n",
      "sensacional\n",
      "solar\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "frequence_terms = nltk.FreqDist(final_words)\n",
    "\n",
    "for word in frequence_terms.most_common(10):\n",
    "    print word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'MiguelCampos12', 4), (u'dudalbuquerqu40', 2), (u'Wallacefla1981', 2), (u'capteinswag', 2), (u'Delatorres10', 2), (u'tmlinsonxx', 1), (u'silva_viviann', 1), (u'Brenda__Evelin', 1), (u'insanomeo', 1), (u'jooaovpereiraa', 1)]\n",
      "[(u'@cbf_futebol', 517), (u'@uolesporte', 212), (u'@uol', 212), (u'@dan_nepomuceno', 184), (u'@italoaraujo08', 146), (u'@youtube', 98), (u'@catleticomg', 95), (u'@whyalef', 81), (u'@rockfla81', 32), (u'@deldrd', 32)]\n",
      "[(u'#onedirection', 135), (u'#mpn', 135), (u'#copapasion', 100), (u'#ariasjustinbieber', 80), (u'#globoesporte', 61), (u'#louisfav', 21), (u'#lideran\\xe7aemjogo', 8)]\n"
     ]
    }
   ],
   "source": [
    "# Retornando os usuários, usuários citados e hashtags mais frequentes\n",
    "frequence_users = nltk.FreqDist(list_user)\n",
    "frequence_users_cited = nltk.FreqDist(users_cited)\n",
    "frequence_hashtags = nltk.FreqDist(hashtags)\n",
    "\n",
    "print(frequence_users.most_common(10))\n",
    "print(frequence_users_cited.most_common(10))\n",
    "print(frequence_hashtags.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abriu', 'teto')\n",
      "('comemorar', 'galera')\n",
      "('futebol', 'sensacional')\n",
      "('galera', 'pleno')\n",
      "('leilton', 'abriu')\n",
      "\n",
      "('abriu', 'teto', 'solar')\n",
      "('comemorar', 'galera', 'pleno')\n",
      "('galera', 'pleno', 'transito')\n",
      "('leilton', 'abriu', 'teto')\n",
      "('pleno', 'transito', 'futebol')\n"
     ]
    }
   ],
   "source": [
    "#Pegando os bigram e trigram mais frequentes\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(final_words)\n",
    "tcf = TrigramCollocationFinder.from_words(final_words)\n",
    "\n",
    "bcf.apply_freq_filter(3)\n",
    "tcf.apply_freq_filter(3)\n",
    "\n",
    "#result_bi = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
    "#result_tri = tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)\n",
    "\n",
    "result_bi = bcf.nbest(BigramAssocMeasures.raw_freq, 5)\n",
    "result_tri = tcf.nbest(TrigramAssocMeasures.raw_freq, 5)\n",
    "\n",
    "\n",
    "for r in result_bi:\n",
    "    print(r)\n",
    "print\n",
    "for r in result_tri:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso finalizamos nossa parte de pré-processamento de texto. Observe que só em extrair de forma correta os termos mais frequentes temos uma visão melhor do que está sendo discutido nas redes sociais. No entanto, exibir isso somente na linha de comando não é muito atrativo. O próximo passo é exibir estas informações em uma página web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
