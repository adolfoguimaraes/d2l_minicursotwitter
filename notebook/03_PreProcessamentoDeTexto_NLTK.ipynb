{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minicurso: O que o twitter está pensando?\n",
    "\n",
    "Estes tutoriais apresentam os principais scritps desenvolvidos no minicurso: **O que o twitter está pensando? Extraindo informações em redes sociais utilizando Python**. Os arquivos completos dos scritps e códigos gerados podem ser encontrados nas pastas **scritps** e **web** na raiz do repositório.\n",
    "\n",
    "A apresentação referente a este minicurso está disponível no site: http://www.data2learning.com/cursos.\n",
    "\n",
    "## 03 - Pré-processamento de texto utilizando NLTK\n",
    "\n",
    "Nesta etapa vamos aprender algumas técnicas para tratar o texto que foi coletado no twitter. Incialmente vamos trabalhar com a parte de tokenização, remoção de palavras que não serão úteis, aplicação de métodos de *steammig* e retirada automática de links, hashtags do texto e outras informaçÕes relevantes que vão ser detalhadas mais a frente.\n",
    "\n",
    "O NLTK (http://www.nltk.org) não é uma ferramenta de pré-processamento, ele é um kit completo para tratar problemas relacionados ao processamento de Linguagem Natural. No entanto, ele possui alguns métodos para nos ajudar nestas tarefas iniciais. Citanto o site oficial: *NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.*.\n",
    "\n",
    "O material desenvolvido aqui é baseado, principalmente, no livro **Python 3 Text Processing with NLTK 3 Cookbook**: https://www.amazon.com.br/dp/B00N2RWMJU/.\n",
    "\n",
    "Vamos começar :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do NLTK\n",
    "\n",
    "Para instalar utilize o comando **pip install nltk** na linha de comando. Esse comando garante a instalação básica da ferramenta. No entanto, o NLTK é composto por uma série de bases, corporas e modelos que podem ser baixados a medida que for sendo utilizando. Detalhes de como instalar os dados podem ser encontrados em: http://nltk.org/data.html. A medida que a gente for precisando destes dados, iremos instalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização\n",
    "\n",
    "A primeira etapa do pré-processamento do texto coletado é separá-lo em *tokens*. Tokenizar consiste em quebrar um texto em pedaços, sejam palavras ou sentenças. Para utilizar os métodos desta etapa será necessário instalar alguns itens adicionais do NLTK.\n",
    "\n",
    "Para isso, digite na linha de comando:\n",
    "\n",
    "\n",
    "$ python\n",
    "\n",
    "\n",
    "No shell do Python digite os comandos a seguir:\n",
    "\n",
    "```python\n",
    ">>> import ntlk\n",
    ">>> nltk.download()\n",
    "NLTK Downloader                                                                                                                                       \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit                                                                                \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "Downloader> \n",
    "```\n",
    "\n",
    "Nessa tela, se digitar **l** será listado todos os pacotes que podem ser instalados. Para o nosso propósito de tokenização vamos instalar o pacote **punkt** que possui os modelos necessários para que a gente faça os diversos tipos de tokenização.\n",
    "\n",
    "```python\n",
    "Downloader> d punkt                                                                                                                                   \n",
    "    Downloading package punkt to /home/d2l/nltk_data...                                                               \n",
    "        Unzipping tokenizers/punkt.zip. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir utiliza um modelo simples de tokenização baseado nas palavras. Observer que ele considera as potuações como uma palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'que', 'o', 'twitter', 'esta', 'pensando', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"O que o twitter esta pensando.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o mesmo método em outra frase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Para', 'mais', 'informacoes', ',', 'acesse', ':', 'http', ':', '//www.unit.br/seminfo']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"Para mais informacoes, acesse: http://www.unit.br/seminfo\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou ainda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'nosso', 'perfil', '@', 'data2learning', '.', 'Estamos', 'monitorando', 'as', 'redes', 'sociais', ',', 'use', ':', '#', 'seminfo', ',', '#', 'seminfoUNIT', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"Siga nosso perfil @data2learning. Estamos monitorando as redes sociais, use: #seminfo, #seminfoUNIT.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O NLTK possui uma série de métodos de tokenização: *PunktWordTokenizer*, *RegexpTokenizer*, *TreebankWordTokenizer*. Cada um destes métodos utilizam abordagens diferentes para separar os tokens. Devemos escolher o método mais adequado a depender do objtivo que queremos alcançar.\n",
    "\n",
    "No nosso caso, observe que objetivo foi em parte atingido. Algumas partes da tokenização não foram bem sucedidas. Termos como links, hashtags e nomes de usuários não deviam ser separados dos 'http', '@' ou '#' correspondentes. Para resolver esse problema, podemos utilizar uma tokenização baseada em expressão regular. Para este tipo, escrevemos uma expressão regular que permita reconhecer os padrões que queremos separar. \n",
    "\n",
    "O NLTK possui o *RegexpTokenizer* para fazer tal tarefa. Esse método faz o que a função básica do python *re.findall()* faz. No entanto, alguns métodos do NLTK exigem que seja definido um expressão regular para reconhecer determinados padrões de texto. Ter uma forma própria de implementar as expressões regulares facilita na definição destes métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir quebra uma frase a partir da expressão regular **[\\w]+**. A expressão diz que o padrão reconhecido deve ser formado por uma ou mais letras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'que', 'o', 'twitter', 'esta', 'pensando']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "expression = \"[\\w]+\"\n",
    "text = \"O que o twitter esta pensando.\"\n",
    "patterns = regexp_tokenize(text, expression)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o ponto não foi selecionado nesse tipo de tokenização já que ele não casa com o padrão desejado. Se utilizarmos esse mesmo padrão nos textos que possuem links, hashtags ou nome de usuários ainda não vamos obter o desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'nosso', 'perfil', 'data2learning', 'Estamos', 'monitorando', 'as', 'redes', 'sociais', 'use', 'seminfo', 'seminfoUNIT']\n"
     ]
    }
   ],
   "source": [
    "text = \"Siga nosso perfil @data2learning. Estamos monitorando as redes sociais, use: #seminfo, #seminfoUNIT.\"\n",
    "patterns = regexp_tokenize(text, expression)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos elaborar um padrão mais completo. Cada \"casamento\" do padrão será reconhecido como um token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'nosso', 'perfil', '@data2learning', 'Estamos', 'monitorando', 'as', 'redes', 'sociais', 'use', '#seminfo', '#seminfoUNIT']\n"
     ]
    }
   ],
   "source": [
    "#Mudando o padrão\n",
    "\n",
    "pattern = r'(https://[^\"\\' ]+|www.[^\"\\' ]+|http://[^\"\\' ]+|\\w+|\\@\\w+|\\#\\w+)'\n",
    "\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A documentação para formar expressões regulares pode ser encontrada na biblioteca **re** do Python: \n",
    "\n",
    "* https://docs.python.org/2/library/re.html (Python 2.7+)\n",
    "* https://docs.python.org/3/library/re.html (Python 3+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Uma outra etapa de pré-processamento é a de eliminação das stopwords. **Stopwords** são palavras comuns que de alguma forma não contribuem para o entendimento da senteça. Vale reforçar que isso vai depender do propósito da sua aplicação. Para o nosso caso, podemos eliminar sem problemas.\n",
    "\n",
    "O NLTK vem com uma lista de stopwords para diversas linguagens. Para poder utilizar esse pacote devemos instalar também utilizando **nltk.download()** exemplificado anteriormente.\n",
    "\n",
    "```python\n",
    ">>> import nltk                                                                                                                                       \n",
    ">>> nltk.download()                                                                                                                                   \n",
    "NLTK Downloader                                                                                                                                       \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit                                                                                \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "Downloader> d stopwords                                                                                                                               \n",
    "    Downloading package stopwords to /home/d2l/nltk_data...                                                                                           \n",
    "      Unzipping corpora/stopwords.zip.    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas listas de palavras. Linguagens disponíveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'danish', u'dutch', u'english', u'finnish', u'french', u'german', u'hungarian', u'italian', u'kazakh', u'norwegian', u'portuguese', u'russian', u'spanish', u'swedish', u'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos exibir algumas palavras (para simplificar, iremos exibir apenas 10 de cada conjunto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your']\n",
      "[u'de', u'a', u'o', u'que', u'e', u'do', u'da', u'em', u'um', u'para']\n",
      "[u'de', u'la', u'que', u'el', u'en', u'y', u'a', u'los', u'del', u'se']\n"
     ]
    }
   ],
   "source": [
    "english_stops = stopwords.words(['english'])\n",
    "portuguese_stops = stopwords.words(['portuguese'])\n",
    "spanish_stops = stopwords.words(['spanish'])\n",
    "print(english_stops[:10])\n",
    "print(portuguese_stops[:10])\n",
    "print(spanish_stops[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso podemos eleminar as stopwords do nosso texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'nosso', 'perfil', '@data2learning', 'Estamos', 'monitorando', 'as', 'redes', 'sociais', 'use', '#seminfo', '#seminfoUNIT']\n",
      "['Siga', 'perfil', '@data2learning', 'Estamos', 'monitorando', 'redes', 'sociais', 'use', '#seminfo', '#seminfoUNIT']\n"
     ]
    }
   ],
   "source": [
    "text = \"Siga nosso perfil @data2learning. Estamos monitorando as redes sociais, use: #seminfo, #seminfoUNIT.\"\n",
    "\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "print(patterns)\n",
    "\n",
    "words = [word for word in patterns if word not in portuguese_stops]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo correções\n",
    "\n",
    "Um problema muito comum quando trabalhamos com textos de redes sociais são os frequntes problemas com a escrita das palavras. Dentre vários problemas que podemos identificar, um que acontece com certa frequência é a inclusão de caracteres repetidos em uma palavra para enfatizar alguma coisa. Por exemplo, \"gostei muuuuiiitoooo desse filme\". O computador não sabe diferenciar \"muito\" de \"muuuuiiitoooo\". Para o processamento de texto seria importante que a gente corrigisse tais casos. A classe a seguir usa a biblioteca **re** do python para elimiar tais casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar a classe definida para corrigir algumas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muito\n",
      "voce\n"
     ]
    }
   ],
   "source": [
    "replacer = RepeatReplacer()\n",
    "\n",
    "word = replacer.replace('muuuuiiitoooo')\n",
    "\n",
    "print(word)\n",
    "\n",
    "word = replacer.replace('voceeeee')\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um outro problema é o fato das pessoas escreverem palavras erradas. Podemos consertar alguns problemas utilizando uma ferramenta de correção de textos. Vamos utilizar uma biblioteca *Enchant* através da biblioteca para python **PyEnchant** (http://pythonhosted.org/pyenchant/). \n",
    "\n",
    "Por padrão, a biblioteca não vem com o dicionário para as linguagens: ['de_DE', 'en_AU', 'en_GB', 'en_US', 'fr_FR']. É necessário instalar o dicionário em Português.\n",
    "\n",
    "Baxei o pacote .deb o MySpell no link:\n",
    "\n",
    "http://packages.ubuntu.com/precise/all/myspell-pt-br/download\n",
    "\n",
    "Para extrair os arquivos internos usamos:\n",
    "\n",
    "\n",
    "$ ar vx mypackage.deb\n",
    "\n",
    "Após isso, foi nessário extrair o conteúdo do arquivo **data.tar**\n",
    "\n",
    "Os arquivos referentes ao dicionário estão na pasta: \n",
    "\n",
    "*data/usr/share/hunspell*\n",
    "\n",
    "Os arquivos pt_BR.aff e pt_BR.dic devem ser copiados na pasta do pacote pyenchant no Python.\n",
    "\n",
    "Vamos criar uma nova classe chamada de SpellingReplacer para corrigir o texto.\n",
    "\n",
    "** Toda essa configuração foi feita para que pudesse funcionar no Mac já que ele não achou os dicionários no sistema. Acredito que no Linux ele funcione automaticamente. Sugiro que antes fazer essa configuração, instale apenas o PyEnchant e veja se funciona. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "    \n",
    "    def __init__(self, dict_name='pt_BR',max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "        \n",
    "    def replace(self, word):\n",
    "        \n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        \n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        \n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caixa\n",
      "você\n"
     ]
    }
   ],
   "source": [
    "replacer = SpellingReplacer()\n",
    "new_word = replacer.replace('caicha')\n",
    "\n",
    "print(new_word)\n",
    "\n",
    "new_word = replacer.replace('voce')\n",
    "\n",
    "print(new_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos mais frequentes\n",
    "\n",
    "Todos os processos feitos até aqui tem como finalidade ajustar as palavras para que a gente possa extrair de forma correta os termos mais frequentes. Selecionar os termos frequentes permitem a gente ter uma visão dos principais termos que estão presentes em um texto. O NLTK possui um método que retorna para a gente a frequencia de cada palavrada, dado um conjunto de palavras.\n",
    "\n",
    "As tarefas feitas anteriomente faz com que **voce** e **voceeeee** sejam tratadas como a mesma palavra. O que de fato é (para o escopo que estamos trabalhando). A mesma coisa acontece para palavras escritas de forma errada.\n",
    "\n",
    "Uma outra coisa que é importante fazer antes de determinar a frequencia das palavras é a retirada de acentos e a normalizar todas as palavras com letra minúscula. \n",
    "\n",
    "O código a seguir faz estas tarefas e calcula a frequência dos termos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#seminfo', 3), ('as', 2), ('siga', 1), ('faz', 1), ('@data2learning', 1), ('quero', 1), ('pessoal', 1), ('estamos', 1), ('participar', 1), ('use', 1), ('monitorando', 1), ('minicursos', 1), ('la', 1), ('sociais', 1), ('na', 1), ('perfil', 1), ('palestras', 1), ('nosso', 1), ('todos', 1), ('vamos', 1), ('redes', 1), ('daqui', 1), ('comecam', 1), ('de', 1), ('como', 1), ('pouco', 1), ('a', 1), ('os', 1), ('#seminfounit', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Importar pacote para normalizar o text\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "\n",
    "# Textos a serem tratados\n",
    "text1 = \"Siga nosso perfil @data2learning. Estamos monitorando as redes sociais, use: #seminfo, #seminfoUNIT.\"\n",
    "text2 = \"Quero participar de todos os minicursos. Como faz? #seminfo\"\n",
    "text3 = \"Vamos lá pessoal, as palestras começam daqui a pouco na #seminfo.\"\n",
    "\n",
    "# Concatenação dos textos\n",
    "text = text1 + \"\\n\" + text2 + \"\\n\" + text3\n",
    "\n",
    "# Retirada dos acentos\n",
    "text = normalize('NFKD', text.decode('utf-8')).encode('ASCII','ignore')\n",
    "\n",
    "# Colaca todas as palavras em minúscula\n",
    "text = text.lower()\n",
    "\n",
    "# Processo de tokenização\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "# Cálculo dos termos mais frequentes\n",
    "frequence_terms = nltk.FreqDist(patterns)\n",
    "    \n",
    "#Podemos ordenar os termos de acordo com a frequência    \n",
    "print(frequence_terms.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "Quando trabalhamos com texto algumas palavras tendem a aparecer juntas. Por exemplo, *United States*, *Barak Obama*, *Hermes Fontes*, *Augusto Franco* e assim por diante. Identificar esses termos é importante dentro do processamento de linguagem natural. Por exemplo, *Hermes Fontes* é um nome de avenida. Desta forma, estas palavras quando aparecem juntas trazer muito mais informação do que as palavras soltas: hermes, fontes. O nome geral para isto é *n-gram*, onde *n* é o número de palavras que vão aparecer juntas. Os mais utilizados são com n = 2 ou n = 3. Estes casos recebem o nome de bigram e trigram, respectivamente.\n",
    "\n",
    "Vamos trabalhar com as frases vistas anteriomente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#seminfounit', 'quero'), ('@data2learning', 'estamos'), ('a', 'pouco'), ('comecam', 'daqui')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(patterns)\n",
    "\n",
    "result = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma podemos implementar para o trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#seminfounit', 'quero', 'participar'), ('@data2learning', 'estamos', 'monitorando'), ('a', 'pouco', 'na'), ('comecam', 'daqui', 'a'), ('daqui', 'a', 'pouco')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(patterns)\n",
    "\n",
    "result = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importância em selecionar estes padrões pode ser enxergado melhor com uma base muito maior de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso encerramos nossa etapa de pré-processamento. Existe muito mais coisas que podem ser feitas como *steamming*, *identificação de sinônimos*, entre outros. Um guia completo destas tarefas podem ser encontradas no livo do NLTK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
